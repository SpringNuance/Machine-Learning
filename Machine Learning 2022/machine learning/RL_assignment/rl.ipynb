{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3f1f2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a049723007a03f29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise about Reinforcement Learning\n",
    "(v.1.1)\n",
    "![Frozen lake](https://debuggirl.files.wordpress.com/2013/03/img_43081.jpg \"Frozen lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7807c3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a01a7f187b223318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Frozen Lake game\n",
    "The Frozen Lake game is about safely navigating a frozen lake whose ice happen have several hidden cracks or holes in it, stepping on any of these will result in you falling into the water. The goal is to reach a part of the lake where a hidden treasure is located. Both the weak spots (holes) and the goal are not known beforehand by the player. Here we will train an *agent* to navigate this lake safely and reach the goal. To do so we will be using the **Q-learning** algorithm. Here we use the **Q-value function** $Q({s, a}) = v$ that takes as input the current state $s$ and possible action $a$, and returns its value $v$. The frozen lake is represented as a grid of cells (gridworld), where each cell is associated with a state that can be start `S`, frozen lake `F`, hole in the ice `H`, or goal `G`, and the actions available at each state is to navigate *left*, *down*, *right*, or *up*.<br>\n",
    "<br>\n",
    "When playing this game the agent will be iteratively doing the following:\n",
    "1. Choose an action given the current state of the agent (calculate where to move).\n",
    "2. Perform the chosen action (move to the chosen cell).\n",
    "3. Upadete the Q-value function (the Q-table) based on the reward from performing this action.\n",
    "4. Set the new state of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90a5eb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e03448fef9ecb599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12aed526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the FrozenLake game\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a12463",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41c7a5b241740882",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Next we render the game board\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941c82c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54b0db6bd3f5b3a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After running the above, you should should see the gameboard, i.e. the frozen lake.\n",
    "<br>\n",
    "SFFF</br>\n",
    "FHFH<br>\n",
    "FFFH<br>\n",
    "HFFG<br>\n",
    "\n",
    "\n",
    "It consist of the following cells:\n",
    " - S - Start.\n",
    " - F - Frozen lake, which is safe to walk on.\n",
    " - H - Hole in the ice. Reaching this cell ends the round.\n",
    " - G - Goal. Reaching this cell gives +1 point and ends the round.\n",
    "\n",
    "As mentioned, we will now train an agent to navigate this frozen lake and ultimately reach the goal (`G` cell) with as few steps/turns as possible without falling into the lake (stepping on any of the `H` cells). At start the agent does not know where the goal is or what cells are safe to walk on. However, with repeated trials, combined with the delayed feedback reward gained from stepping on the various cells, the agent should eventually learn to navigate this lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11f6b3f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1ed9720d13e6350",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frozen lake (gameboard) consist of 16 states (cells) and 4 actions for each of these.\n",
      "So for each state there are four actions availble:\n",
      "0 = ‚óÄÔ∏è left\n",
      "1 = üîΩ down\n",
      "2 = ‚ñ∂Ô∏è right\n",
      "3 = üîº up\n"
     ]
    }
   ],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "print('The frozen lake (gameboard) consist of {} states (cells) and {} actions for each of these.'.format(state_space_size, action_space_size))\n",
    "print('So for each state there are four actions availble:\\n0 = ‚óÄÔ∏è left\\n1 = üîΩ down\\n2 = ‚ñ∂Ô∏è right\\n3 = üîº up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734a667",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dba9577676343545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q-table\n",
    "We will be using a **Q-table** as the underlying representation of the value function (Q-value function). For this type of game board, the Q-table is a well suited representation to use. This table will store all the knowledge gained by the agent - each *state* will have information about the *value* of taking any of the available *actions* (‚óÄÔ∏è, üîΩ, ‚ñ∂Ô∏è, üîº) when the policy is to find the hidden treasure without falling into the lake. To start with, all the q-values in this table will be zero, but these will be updated as the agent is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b848f4b1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3905c01e39ae4d2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[0. 0. 0. 0.]\t<-- state [0,0]\n",
      "[0. 0. 0. 0.]\t<-- state [0,1]\n",
      "[0. 0. 0. 0.]\t<-- state [0,2]\n",
      "[0. 0. 0. 0.]\t<-- state [0,3]\n",
      "[0. 0. 0. 0.]\t<-- state [1,0]\n",
      "[0. 0. 0. 0.]\t<-- state [1,1]\n",
      "[0. 0. 0. 0.]\t<-- state [1,2]\n",
      "[0. 0. 0. 0.]\t<-- state [1,3]\n",
      "[0. 0. 0. 0.]\t<-- state [2,0]\n",
      "[0. 0. 0. 0.]\t<-- state [2,1]\n",
      "[0. 0. 0. 0.]\t<-- state [2,2]\n",
      "[0. 0. 0. 0.]\t<-- state [2,3]\n",
      "[0. 0. 0. 0.]\t<-- state [3,0]\n",
      "[0. 0. 0. 0.]\t<-- state [3,1]\n",
      "[0. 0. 0. 0.]\t<-- state [3,2]\n",
      "[0. 0. 0. 0.]\t<-- state [3,3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "# Print some info about it\n",
    "s = 'Q-table:\\n'\n",
    "i = 0\n",
    "j = 0\n",
    "for cr in q_table:\n",
    "    s += '{}\\t<-- state [{},{}]\\n'.format(cr, i, j)\n",
    "    j += 1\n",
    "    if j >= action_space_size:\n",
    "        j = 0\n",
    "        i += 1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae01632",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44427ed4e698c56b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With an empty Q-table the agent do not know anything about what actions are better than others. In other words, the Q-value function will always return zero for every possible action at each state. Thus in the beginning the agent will instead be taking random actions while exploring the board. This is called *exploration*. However, after having gained some knowledge about the board, the agent can start relying on the experience gained and reflected in the Q-value function. This is called *exploitation*. We will be using the *epsilon greedy strategy* to find a balance between *exploration* (random actions) and *exploitation* (optional actions according to the Q-value function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62e5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we introduce some parameters to use in the Q-training algorithm.\n",
    "\n",
    "# Learning rate (value between 0 and 1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Reward discount used in the Bellman equation (value between 0 and 1)\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44db8b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a040c9a8a6c9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Bellman equation\n",
    "For updating a state in the Q-table we will here be using the following version of the *Bellman equation*:<br>\n",
    "$$Q^{updated}(s_{t}, a_{t}) := (1-\\alpha)Q(s_{t}, a_{t}) + \\alpha (r_{t} + \\gamma \\underset{a}{\\max}Q(s_{t+1},a))$$\n",
    "$s_{t}$ - current state.<br>\n",
    "$a_{t}$ - chosen action.<br>\n",
    "$\\alpha$ - learning rate.<br>\n",
    "$r_{t}$ - reward from taking the action $a_{t}$.<br>\n",
    "$\\gamma$ - discount factor.<br>\n",
    "$s_{t+1}$ - new state after taking action $a_{t}$.<br>\n",
    "$\\underset{a}{\\max}Q(s_{t+1},a)$ - the optional value possible from $s_{t+1}$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80054275",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da18419fc78cb4a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.1\n",
    "\n",
    "<strong><i><u>Your task<i></u></strong> is to implement the Bellman equation as a function that calculates and returns the new value for $Q^{updated}(s_{t}, a_{t})$.<br>It takes as input: Q-table, $s_{t}$, $a_{t}$, $s_{t+1}$, $r_{t}$, $\\alpha$, and $\\gamma$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1915777",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b8c88159a873820",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor):\n",
    "    \"\"\"\n",
    "    Calculate the new value in the q_table using Bellman equation\n",
    "    \"\"\"\n",
    "    ## Implement the Bellman equation as shown above\n",
    "    # q_new = q_table[state, action] ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    q_new = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc8915d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-789d006652c57c07",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Should this test be improved somehow?\n",
    "assert q_update(q_table=q_table, state=0, action=2, new_state=1, reward=0, learning_rate=learning_rate, discount_factor=discount_factor) == 0.0\n",
    "assert q_update(q_table=q_table, state=0, action=2, new_state=1, reward=1, learning_rate=learning_rate, discount_factor=discount_factor) == 0.1\n",
    "\n",
    "test_q_table = np.array([[0.58940116, 0.50458308, 0.50449804, 0.50430707],\n",
    "                [0.39701686, 0.31072888, 0.32279088, 0.52844835],\n",
    "                [0.40813145, 0.43028301, 0.41980598, 0.4987701],\n",
    "                [0.34803986, 0.31594787, 0.31342393, 0.47886813],\n",
    "                [0.6137157, 0.39051021, 0.33313007, 0.40253577],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45621221, 0.157744,  0.16167418, 0.15956471],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.40419588, 0.44695664, 0.49716483, 0.66166421],\n",
    "                [0.48304007, 0.70040278, 0.43290747, 0.42981004],\n",
    "                [0.68873522, 0.45787092, 0.36492453, 0.39246048],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45736412, 0.66062829, 0.80533251, 0.59763036],\n",
    "                [0.7685619,  0.9257641,  0.77140422, 0.75617869],\n",
    "                [0.,         0.,         0.,         0.        ]])\n",
    "assert q_update(q_table=test_q_table, state=8, action=2, new_state=9, reward=0, learning_rate=learning_rate, discount_factor=discount_factor) == 0.51678822222\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mentioned epsilon greedy strategy we will need a few more parameters.\n",
    "\n",
    "# Upper bound for the exploration rate (value between 0 and 1)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Lower bound for the exploration rate (value between 0 and 1)\n",
    "min_exploration_rate = 0.01  # 0.01\n",
    "\n",
    "# How much the exploration rate decays per episode (down to the min_exploration_rate, value between 0 and 1).\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd219d",
   "metadata": {},
   "source": [
    "### Epsilon greedy strategy\n",
    "By using the *epsilon greedy strategy*, we will let the exploration rate start high (1), and then decrease for every episode with exponential decay. The goal here is to find the optimal balance between exploration and exploitation. Each round will consist of 10000 episodes. In the first episodes we want to have a high probility of performing exploration actions to avoid getting stuck in sub-optimal paths (local maximums). However, we want to gradually start leaning more towards exploitation towards the later episodes.<br>\n",
    "<br>\n",
    "We will be using the following formula for calculating the exploration rate for a given episode $\\epsilon(i)$:<br>\n",
    "$$\\epsilon(i) = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min}) \\cdot e^{-\\zeta \\cdot i}$$\n",
    "$\\epsilon$ - exploration rate.<br>\n",
    "$i$ - episode number.<br>\n",
    "$\\epsilon_{min}$ - min exploration rate.<br>\n",
    "$\\epsilon_{max}$ - max exploration rate.<br>\n",
    "$\\zeta$ - exploration decay rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcc3f5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd5341b2e6ce6da4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.2\n",
    "\n",
    "<strong><i><u>Your task<i></u></strong> to implement the `calc_exploration_rate` function which calculates and returns the exploration rate using the above formula.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210d77f4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d10be506d65f993",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculate the exploration rate for a given episode\n",
    "def calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate):\n",
    "    ## Calculate the exploration rate\n",
    "    # exploration_rate = ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    ### END SOLUTION\n",
    "    return exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df34e84c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-307f265b7d244b66",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert calc_exploration_rate(700, 0.01, 1, 0.005) == 0.039895409588095315\n",
    "assert calc_exploration_rate(100, 0.01, 1, 0.005) == 0.6104653531155071\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb551b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5007d85f0ea8352f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.3\n",
    "\n",
    "Next we will make the function for choosing an action given a state. This function includes an *exploration rate threshold* which is a randomly drawn number between 0 and 1. For every action, this thershold makes the agent to perform an exploration action with a probability equal to the given *exploration rate*.\n",
    "<br>\n",
    "<strong><i><u>Your task<i></u></strong> is to complete the below function `choose_action`. This function takes as input the Q-table, $s_{t}$, and exploration rate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f932326",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-188c8ae072cecd71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, exploration_rate):\n",
    "    ## Pick a random number between 0 and 1\n",
    "    # exploration_rate_threshold = ...\n",
    "    \n",
    "    ### BEGIN SOLUTION \n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    ## Exploration vs exploitation\n",
    "    # if exploration_rate > exploration_rate_threshold or sum(q_table[state, :]) == 0:\n",
    "        ## Exploration, i.e., random action        \n",
    "        # action = ...\n",
    "    # else:\n",
    "        ## Exploitation, i.e., perform the optimal action according to the Q-table\n",
    "        # action = ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    if exploration_rate > exploration_rate_threshold or sum(q_table[state, :]) == 0:\n",
    "        ## Exploration, i.e., random action \n",
    "        action = random.choice(range(len(['left', 'down', 'right', 'up'])))\n",
    "        # We could also use: action = env.action_space.sample()\n",
    "    else:\n",
    "        ## Exploitation, i.e., perform the optimal action according to the Q-table\n",
    "        action = np.argmax(q_table[state, :])\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5411b02b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c5bab8432aab8bc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "test_q_table = np.array([[0.58940116, 0.50458308, 0.50449804, 0.50430707],\n",
    "                [0.39701686, 0.31072888, 0.32279088, 0.52844835],\n",
    "                [0.40813145, 0.43028301, 0.41980598, 0.4987701],\n",
    "                [0.34803986, 0.31594787, 0.31342393, 0.47886813],\n",
    "                [0.6137157, 0.39051021, 0.33313007, 0.40253577],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45621221, 0.157744,  0.16167418, 0.15956471],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.40419588, 0.44695664, 0.49716483, 0.66166421],\n",
    "                [0.48304007, 0.70040278, 0.43290747, 0.42981004],\n",
    "                [0.68873522, 0.45787092, 0.36492453, 0.39246048],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45736412, 0.66062829, 0.80533251, 0.59763036],\n",
    "                [0.7685619,  0.9257641,  0.77140422, 0.75617869],\n",
    "                [0.,         0.,         0.,         0.        ]])\n",
    "\n",
    "action_list = []\n",
    "for i in range(100):\n",
    "    i_action = choose_action(test_q_table, 3, 0)\n",
    "    action_list.append(i_action)\n",
    "assert action_list[0] == 3 and all(x == action_list[0] for x in action_list)\n",
    "\n",
    "action_list = []\n",
    "for i in range(100):\n",
    "    i_action = choose_action(test_q_table, 3, 1)\n",
    "    action_list.append(i_action)\n",
    "assert not all(x == action_list[0] for x in action_list)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80ede",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8de9391ad7cfd1d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q-Learning algorithm\n",
    "Now it is time to train our agent using the Q-learning algorithm as shown below.\n",
    "If everything works as intended, the below code should print some statistics from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d85fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes to train\n",
    "num_episodes = 10000\n",
    "\n",
    "# Max number of steps before ending the round\n",
    "max_steps_per_episode = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0803db2f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc1b4d892ae491bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results, average reward and steps per thousand episodes:\n",
      "1000: rewards 0.257, steps 8.017\n",
      "2000: rewards 0.734, steps 6.831\n",
      "3000: rewards 0.905, steps 6.326\n",
      "4000: rewards 0.945, steps 6.091\n",
      "5000: rewards 0.975, steps 6.072\n",
      "6000: rewards 0.983, steps 6.032\n",
      "7000: rewards 0.987, steps 6.029\n",
      "8000: rewards 0.989, steps 6.027\n",
      "9000: rewards 0.992, steps 6.044\n",
      "10000: rewards 0.99, steps 6.044\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning algorithm\n",
    "rewards_all_episodes = []\n",
    "steps_all_episodes = []\n",
    "exploration_rate = max_exploration_rate\n",
    "\n",
    "# Play the game for n episodes\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    steps_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Chooce an action. Exploration-exploitation trade-off with the epsilon greedy strategy\n",
    "        action = choose_action(q_table, state, exploration_rate)\n",
    "        \n",
    "        # Perform the chosen action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Upadete the Q-table\n",
    "        q_table[state, action] = q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor)\n",
    "        \n",
    "        # Set the new state\n",
    "        state = new_state\n",
    "        \n",
    "        # Collect some statistics\n",
    "        rewards_current_episode += reward\n",
    "        steps_current_episode += 1\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # Update the exploration rate\n",
    "    exploration_rate = calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    steps_all_episodes.append(steps_current_episode)\n",
    "\n",
    "\n",
    "# Print the average rewards\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "steps_per_thousand_episodes = np.split(np.array(steps_all_episodes), num_episodes/1000)\n",
    "\n",
    "print('Training results, average reward and steps per thousand episodes:')\n",
    "count = 1000\n",
    "for i, _ in enumerate(rewards_per_thousand_episodes):\n",
    "    r = rewards_per_thousand_episodes[i]\n",
    "    s = steps_per_thousand_episodes[i]\n",
    "    print('{}: rewards {:.4}, steps {:.4}'.format(count, sum(r/1000), sum(s/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7344263d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f074e791805dc94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[0.94148015 0.93206535 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099005 0.96059601]\n",
      " [0.96059601 0.         0.86161114 0.9060303 ]\n",
      " [0.88425553 0.75878929 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059597]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3574754  0.         0.93396316 0.48765719]\n",
      " [0.58939914 0.89163075 0.98009975 0.        ]\n",
      " [0.97029515 0.99       0.         0.97029897]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.75452119 0.98999972 0.72104627]\n",
      " [0.98009562 0.98999955 1.         0.98009987]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Trained Q-table:')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622d7c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5b976692e3ed316",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Playing Frozen Lake\n",
    "Finally we will let the agent play Frozen Lake using the Q-table that we have learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447edd02",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff4b832693eb4e4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.4\n",
    "\n",
    "Now we are ready to put the agent to navigate the frozen lake based on what it has learned. Maybe you can, by looking at the Q-table already see what route the agent will be taking?<br><br>\n",
    "<strong><i><u>Your task<i></u></strong> is to run the below code and observe how the agent is now navigating the frozen lake. Observe the learned Q-table to understand why the agent is taking this specific path.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3849f1a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b659ed92114c510e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡º∫‚ùÅ Done ‚ùÅ‡ºª\n"
     ]
    }
   ],
   "source": [
    "# Watch the agent play for a few rounds using the learned Q-table\n",
    "n_rounds = 3\n",
    "for episode in range(n_rounds):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    print('‡º∫‚ùÅ EPISODE {} ‚ùÅ‡ºª'.format(episode+1))\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode + 1):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        action = np.argmax(q_table[state, :])\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print('‡º∫‚ùÅ You reached the goal after {} steps! ‚ùÅ‡ºª'.format(step + 1))\n",
    "                time.sleep(3)\n",
    "            elif step == max_steps_per_episode - 1:\n",
    "                print('‡º∫‚ùÅ No steps left ({} steps max)! ‚ùÅ‡ºª'.format(step + 1))\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('‡º∫‚ùÅ You fell through a hole after {} steps! ‚ùÅ‡ºª'.format(step + 1))\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        elif step == max_steps_per_episode - 1:\n",
    "            print('‡º∫‚ùÅ No steps left ({} steps max)! ‚ùÅ‡ºª'.format(step + 1))\n",
    "            time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state\n",
    "time.sleep(3)\n",
    "print('‡º∫‚ùÅ Done ‚ùÅ‡ºª')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d9514",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9a443a782d5348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The ice can get slippery!\n",
    "![Slippery ice](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7iaMa5OcNHtJFKB1fZo_EvG-nQyPPTm_fsA&usqp=CAU \"Slippery ice\")\n",
    "\n",
    "So far the ice has been easy to walk on, and we have seen that the agent has not had any problem in finding an optimal path leading to the goal. Now, however, the weather has changed and the ice has suddenly become very slippery.\n",
    "In the next part of this exercise we want you to go to the top/beginning of this notebook to the cell where we initate the Frozen Lake environment:<br>\n",
    "`env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")`<br>\n",
    "Now change `is_slippery=False` to `is_slippery=True`.<br>\n",
    "Setting `is_slippery=True` means that there is only a 1/3 chance that the agent will move in the intended \n",
    "direction, and 2/3 chance to move in one of the perpendicular directions (1/3 and 1/3). \n",
    "E.g., if **action = left**, then:\n",
    "- P(move left) = 1/3\n",
    "- P(move up) = 1/3\n",
    "- P(move down) = 1/3\n",
    "\n",
    "Next, re-run the whole notebook (‚ñ∂‚ñ∂) and observe how the new trained Q-table looks like, how the new statistics has become, and also observe how the behaviour of the agent has changed at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642b7f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c75890d0dc7b593e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.5\n",
    "\n",
    "First make sure to set `is_slippery=True` at the top/beginning of this notebook.<br>\n",
    "<strong><i><u>Your task<i></u></strong> is to briefly explore the impact of different values assigned to the different parameters, and try to find the most promising values that optimize the reward gained by the agent (look at the scores from the last 1000 episodes).\n",
    "The parameters in question are:\n",
    "- learning_rate ($\\alpha$)\n",
    "- discount_factor ($\\gamma$)\n",
    "- max_exploration_rate ($\\epsilon_{max}$)\n",
    "- min_exploration_rate ($\\epsilon_{min}$)\n",
    "- exploration_decay_rate ($\\zeta$)\n",
    "\n",
    "<u>Be prepared to discuss your findings in the exercise lecture.</u><br>\n",
    "Some additional things to think about: Why is the preferred first move of the agent to move left? Can you think of some possible approaches and strategies that could improve the performance of the agent further? Can you think of some completely different ways for how to solve this game/problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4b89b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
