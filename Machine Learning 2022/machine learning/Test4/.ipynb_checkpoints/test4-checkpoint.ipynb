{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6375bed3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50170e1c92e960be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 5\n",
    "(v.1.4)\n",
    "## Lecture 9 & 10 - Reinforcement Learning\n",
    "\n",
    "### Learning Goals\n",
    "After successfully completing this assignment, you should be able to:\n",
    "- explain why exploration and exploitation are important concepts in reinforcement learning problems\n",
    "- understand better the Multi-Armed Bandit problem\n",
    "- implement a strategy for maximizing the cumulative reward (UCB)\n",
    "- understand the Frozen Lake game/problem\n",
    "- implement an algorithm for training an agent to find an optimal path through this grid game (Q-learning)\n",
    "- understand on an intuitive level how different parameters impacts the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0bb5c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f87255cf46066085",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of Contents\n",
    "1. [Multi-Armed Bandit](#multi_armed_bandit)\n",
    "2. [Frozen Lake Game](#frozen_lake_game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cb649",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2d620d526c5f3d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Multi-Armed Bandit <a class=\"anchor\" id=\"multi_armed_bandit\"></a>\n",
    "\n",
    "![Multi-armed bandit](https://leovan.me/images/cn/2020-05-16-multi-armed-bandit/compulsive-gambling.png \"Multi-armed bandit\")\n",
    "\n",
    "Here we will focus on the \"Multi-armed bandit\" problem. There are 10 one-armed bandit machines where each have different probabilities for winning and with different reward ranges. As we have a limited amount of coins to spend, the goal is to play on these machines in a way that maximizes the cumulative reward. An action at every time step involves selecting and playing on  one of these ten one-armed bandits, or simply arms. For each action, i.e. for each arm that we play, we observe the reward which is a value between 0 and 10 (€). Since we know nothing about the reward properties of these machines to start with, we need to implement a suitable strategy that is based around observing the rewards that we get from playing on these machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ea70aa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f2103e7721e7d98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "random.seed(1234)\n",
    "import numpy as np\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4b13",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f735c01ae7155155",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "The dataset that we will use here is `10_armed_bandit_data.csv`. Each row in this dataset shows the reward of playing on each arm at a given time step. With this way of representing the data, the reward from each arm changes for each time step, meaning that also the order in which the arms are selected will influence the outcome. Since there are 1000 rows in the dataset, it means we have 1000 coins to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "116f9956",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc85ecd8866b57b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Arm 0  Arm 1  Arm 2  Arm 3  Arm 4  Arm 5  Arm 6  Arm 7  Arm 8  Arm 9\n",
      "0      0      0      0      4      2      0      0      0      0      1\n",
      "1      0      0      6      0      0      0      0      0      0      0\n",
      "2      0      0      0      0      3      0      0      0      0      0\n",
      "3      0      0      7      0      2      0      1      5      0      0\n",
      "4      0      2      0      5      0      0      0      0      0      0\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_csv('10_armed_bandit_data.csv')\n",
    "\n",
    "# Print the first 5 rows from the dataset.\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195f67c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dafc102f4280ff6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Random Selection\n",
    "In this first part we adopt a simple strategy that involves naively selecting and playing on a random arm each time, independent of the rewards we get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac19a7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e939a12af6e0a84d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.1\n",
    "\n",
    "Your task is to implement the Random Selection strategy (selecting a random arm $a$ at every time step $t$) by completing the `random_selection()` function. Our budget is 1000 coins.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72d79c53",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-545908122b24ea7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward using Random Selection: 834\n",
      "\n",
      "  Arm    Times selected\n",
      "-----  ----------------\n",
      "    6               115\n",
      "    9               114\n",
      "    7               110\n",
      "    5               104\n",
      "    4               102\n",
      "    1                97\n",
      "    8                97\n",
      "    2                93\n",
      "    0                88\n",
      "    3                80\n"
     ]
    }
   ],
   "source": [
    "# Implementing Random Selection\n",
    "def random_selection(data):\n",
    "    \n",
    "    # There are 10 columns in the dataset, one for each arm/machine.\n",
    "    # The row count represents the number of times we play, i.e., how many coins we will spend. \n",
    "    coins, arms = data.shape  # (1000, 10)\n",
    "    \n",
    "    reward_sum_per_arm = [0] * arms\n",
    "    selection_count_per_arm = [0] * arms\n",
    "    \n",
    "    for t in range(0, coins):\n",
    "        \n",
    "        ## Select a random arm (action) among the 10 available arms\n",
    "        # a = random. ...\n",
    "        \n",
    "        ## Fetch the reward for selecting the given arm (a) at the given time step (t) from the data\n",
    "        # reward = data.values[...]\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        a = random.randrange(arms)\n",
    "        reward = data.values[t, a]\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        selection_count_per_arm[a] += 1\n",
    "        reward_sum_per_arm[a] += reward\n",
    "\n",
    "    return reward_sum_per_arm, selection_count_per_arm\n",
    "\n",
    "\n",
    "# Run the selection algorithm\n",
    "reward_sum_per_arm, selection_count_per_arm = random_selection(df)\n",
    "\n",
    "# Print the outcome and some statistics\n",
    "print('Total reward using Random Selection: {}'.format(sum(reward_sum_per_arm)))\n",
    "\n",
    "sorted_indxs = np.argsort([-x for x in selection_count_per_arm])\n",
    "tab_data = [[indx, selection_count_per_arm[indx]] for indx in sorted_indxs]\n",
    "headers = ['Arm', 'Times selected']\n",
    "print('\\n' + tabulate.tabulate(tab_data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "552d33e8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0584325e68b31952",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "# min_reward = 598, max_reward = 985\n",
    "reward_s_p_a, _ = random_selection(df)\n",
    "tot_reward = sum(reward_s_p_a)\n",
    "assert(550 <= tot_reward <= 1000)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3e98e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac3299f40a62e838",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Upper Confidence Bound (UCB) Selection\n",
    "\n",
    "Random selection of arms to play on is probably not the optimal approach. To improve our reward we want to use the UCB algorithm to calculate $a_t$ which is the action, or arm, at time $t$ with maximum UCB:\n",
    "$$a_t = \\underset{a\\in{A}}{\\operatorname{argmax}}\\biggr[Q_t(a) + c \\frac{\\sqrt{\\log{t}}}{N_t(a)}\\biggr]$$\n",
    "<br>\n",
    "Where $Q_t(a)$ is the estimated value of action (arm) $a$ at time step $t$:\n",
    "$$Q_t(a) = \\frac{ \\sum_{i=1}^{t-1} R_{i}\\mathbf{1}_{A_i = a} }{ \\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a} }$$\n",
    "<br>\n",
    "$t$ - current time step.<br>\n",
    "$N_t(a)$ - the number of times that action $a$ has been selected, prior to time $t$.<br>\n",
    "$c$ - confidence value that controls the level of *exploration* vs *exploitation*. Higher $c$ means higher exploration rate (... of arms that have not been explored much or yielded much reward so far).<br>\n",
    "$\\sum_{i=1}^{t-1} R_{i}\\mathbf{1}_{A_i = a}$ - sum of rewards from selecting $a$ up to $t$-1<br>\n",
    "$\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i = a}$ - number of times $a$ has been selected up to $t$-1<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78244e7e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3acd4c88a65b8862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.2\n",
    "\n",
    "In this assignment your task is to implement the above UCB algorithm, `max_ucb_action()`. As inputs our function takes $c$, $t$, a list storing the accumulated rewards per arm (`reward_sum_per_arm`), and a list with informaion about how many times each arm has been selected so far (`selection_count_per_arm`, this is $\\{N_t(A)\\}$).\n",
    "<br><br>\n",
    "Note, in the beginning the value of $N_t(a)$ will be zero. To avoid the division by zero problem, we can, in those cases, set it to a number that is close to zero, e.g., 1e-100.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "483be6fe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7d2a05e1457eb03",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Example showing how the input arguments might look like at t = 40.\n",
    "# c = 1\n",
    "# t = 40\n",
    "# reward_sum_per_arm = [0, 12, 0, 2, 1, 0, 0, 0, 0, 0]\n",
    "# selection_count_per_arm = [3, 15, 3, 5, 4, 3, 2, 2, 2, 2]\n",
    "\n",
    "# reward_sum_per_arm and selection_count_per_arm stores the cummulated values up to the current time step t\n",
    "\n",
    "def max_ucb_action(c, t, reward_sum_per_arm, selection_count_per_arm):\n",
    "    arms = len(reward_sum_per_arm)\n",
    "    confidence_per_arm = [0] * arms\n",
    "    \n",
    "    # In this for-loop we calculate the confidence_per_arm (i.e., action a) at the given time step t\n",
    "    for a in range(0, arms):\n",
    "        \n",
    "        Nt = selection_count_per_arm[a]\n",
    "        if Nt == 0:\n",
    "            Nt = 1e-100  # To avoid division by zero error\n",
    "        \n",
    "        # Qt = ...\n",
    "        \n",
    "        ## NB!: use math.log(t+1) to avoid error with math.log(0)\n",
    "        # Ut = ...  \n",
    "        \n",
    "        # confidence_per_arm[a] = ...\n",
    "        \n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        Qt = reward_sum_per_arm[a] / Nt\n",
    "        Ut = c * math.sqrt(math.log(t+1) / Nt)\n",
    "        confidence_per_arm[a] = Qt + Ut\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    # Pick the arm/action with the highest upper bound\n",
    "    max_upper_bound_arm = confidence_per_arm.index(max(confidence_per_arm))\n",
    "    return max_upper_bound_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "430fc9d7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05d969ea25aaf599",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "c_ = 1\n",
    "t_ = 40\n",
    "reward_sum_per_arm_ = [0, 12, 0, 2, 1, 0, 0, 0, 0, 0]\n",
    "selection_count_per_arm_ = [3, 15, 3, 5, 4, 3, 2, 2, 2, 2]\n",
    "assert(max_ucb_action(c_, t_, reward_sum_per_arm_, selection_count_per_arm_) == 6)\n",
    "\n",
    "c_ = 1\n",
    "t_ = 400\n",
    "reward_sum_per_arm_ = [5, 76, 0, 220, 1, 0, 3, 0, 0, 18]\n",
    "selection_count_per_arm_ = [13, 90, 5, 232, 7, 5, 10, 5, 5, 29]\n",
    "assert(max_ucb_action(c_, t_, reward_sum_per_arm_, selection_count_per_arm_) == 3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f2491",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4c93de7e4fba678",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.3\n",
    "\n",
    "Now that we have implemented `max_ucb_action()`, the next thing to do is to implement the function `ucb_selection()` where we use the UCB selection approach for playing on these 10 one-armed bandits. Our budget is again 1000 coins.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63a328bd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f969fac19bedee18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "Total reward using UCB Selection: 1200\n",
      "\n",
      "  Arm    Times selected\n",
      "-----  ----------------\n",
      "    3               874\n",
      "    9                61\n",
      "    7                35\n",
      "    4                 6\n",
      "    0                 4\n",
      "    1                 4\n",
      "    2                 4\n",
      "    5                 4\n",
      "    6                 4\n",
      "    8                 4\n"
     ]
    }
   ],
   "source": [
    "# Implementing UCB Selection\n",
    "def ucb_selection(data, c):\n",
    "    \n",
    "    ## Implement this in the same way as the above random_selection() function (from A5.1).\n",
    "    ## The only exception is that you should now use use max_ucb_action() to select arms \n",
    "    ## instead of selecting arms at random.\n",
    "    # ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    coins, arms = df.shape  # (1000, 10)\n",
    "    \n",
    "    reward_sum_per_arm = [0] * arms\n",
    "    selection_count_per_arm = [0] * arms\n",
    "    \n",
    "    for t in range(0, coins):\n",
    "        # Select the action/arm maximizing the Upper Confidence Bound (UCB)\n",
    "        a = max_ucb_action(c, t, reward_sum_per_arm, selection_count_per_arm)\n",
    "        \n",
    "        reward = data.values[t, a]\n",
    "        selection_count_per_arm[a] += 1\n",
    "        reward_sum_per_arm[a] += reward\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return reward_sum_per_arm, selection_count_per_arm\n",
    "\n",
    "\n",
    "# Run the selection algorithm (by default we have set c = 1)\n",
    "c = 1\n",
    "reward_sum_per_arm, selection_count_per_arm = ucb_selection(df, c)\n",
    "\n",
    "# Sanity check\n",
    "print(sum(ucb_selection(df, c=1)[0]))\n",
    "assert(sum(ucb_selection(df, c=1)[0]) == 1200)\n",
    "\n",
    "\n",
    "# Print the outcome and some statistics\n",
    "print('Total reward using UCB Selection:', sum(reward_sum_per_arm))\n",
    "\n",
    "sorted_indxs = np.argsort([-x for x in selection_count_per_arm])\n",
    "tab_data = [[indx, selection_count_per_arm[indx]] for indx in sorted_indxs]\n",
    "headers = ['Arm', 'Times selected']\n",
    "print('\\n' + tabulate.tabulate(tab_data, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19ab6bbf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4b9cc6f50f975e03",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "c_ = 1\n",
    "reward_s_p_a_1_, _ = ucb_selection(df, c_)\n",
    "assert(sum(reward_s_p_a_1_) == 1200)\n",
    "\n",
    "c_ = 0.5\n",
    "reward_s_p_a_2_, _ = ucb_selection(df, c_)\n",
    "assert(sum(reward_s_p_a_2_) == 1381)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf959c8e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c08ca7a82d27243e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Bonus Task\n",
    "\n",
    "So far we have used $c=1$, can you find the $c$ value that provides the highest accumulated reward?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75c177",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71dfe6eb8e814916",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e7807c3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a01a7f187b223318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The Frozen Lake Game <a class=\"anchor\" id=\"frozen_lake_game\"></a>\n",
    "<img src=\"https://debuggirl.files.wordpress.com/2013/03/img_43081.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The Frozen Lake game is about safely navigating a frozen lake whose ice happen have several hidden cracks or holes in it, stepping on any of these will result in you falling into the water. The goal is to reach a part of the lake where a hidden treasure is located. Both the weak spots (holes) and the goal are not known beforehand by the player. Here we will train an *agent* to navigate this lake safely and reach the goal. To do so we will be using the **Q-learning** algorithm. Here we use the **Q-value function** $Q({s, a}) = v$ that takes as input the current state $s$ and possible action $a$, and returns its value $v$. The frozen lake is represented as a grid of cells (gridworld), where each cell is associated with a state that can be start `S`, frozen lake `F`, hole in the ice `H`, or goal `G`, and the actions available at each state is to navigate *left*, *down*, *right*, or *up*.<br>\n",
    "<br>\n",
    "When playing this game the agent will be iteratively doing the following:\n",
    "1. Choose an action given the current state of the agent (calculate where to move).\n",
    "2. Perform the chosen action (move to the chosen cell).\n",
    "3. Upadete the Q-value function (the Q-table) based on the reward from performing this action.\n",
    "4. Set the new state of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e90a5eb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e03448fef9ecb599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we will be using\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "\n",
    "# Initiate the FrozenLake game\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")\n",
    "\n",
    "# Next we render the game board\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941c82c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54b0db6bd3f5b3a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After running the above, you should should see the gameboard, i.e. the frozen lake.\n",
    "<br>\n",
    "SFFF</br>\n",
    "FHFH<br>\n",
    "FFFH<br>\n",
    "HFFG<br>\n",
    "\n",
    "\n",
    "It consist of the following cells:\n",
    " - S - Start.\n",
    " - F - Frozen lake, which is safe to walk on.\n",
    " - H - Hole in the ice. Reaching this cell ends the round.\n",
    " - G - Goal. Reaching this cell gives +1 point and ends the round.\n",
    "\n",
    "As mentioned, we will now train an agent to navigate this frozen lake and ultimately reach the goal (`G` cell) with as few steps/turns as possible without falling into the lake (stepping on any of the `H` cells). At start the agent does not know where the goal is or what cells are safe to walk on. However, with repeated trials, combined with the delayed feedback reward gained from stepping on the various cells, the agent should eventually learn to navigate this lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11f6b3f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1ed9720d13e6350",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frozen lake (gameboard) consist of 16 states (cells) and 4 actions for each of these.\n",
      "So for each state there are four actions availble:\n",
      "0 = ◀️ left\n",
      "1 = 🔽 down\n",
      "2 = ▶️ right\n",
      "3 = 🔼 up\n"
     ]
    }
   ],
   "source": [
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "print('The frozen lake (gameboard) consist of {} states (cells) and {} actions for each of these.'.format(state_space_size, action_space_size))\n",
    "print('So for each state there are four actions availble:\\n0 = ◀️ left\\n1 = 🔽 down\\n2 = ▶️ right\\n3 = 🔼 up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734a667",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dba9577676343545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Table\n",
    "We will be using a **Q-table** as the underlying representation of the value function (Q-value function). For this type of game board, the Q-table is a well suited representation to use. This table will store all the knowledge gained by the agent - each *state* will have information about the *value* of taking any of the available *actions* (◀️, 🔽, ▶️, 🔼) when the policy is to find the hidden treasure without falling into the lake. To start with, all the q-values in this table will be zero, but these will be updated as the agent is playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b848f4b1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3905c01e39ae4d2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[0. 0. 0. 0.]\t<-- state [0,0]\n",
      "[0. 0. 0. 0.]\t<-- state [0,1]\n",
      "[0. 0. 0. 0.]\t<-- state [0,2]\n",
      "[0. 0. 0. 0.]\t<-- state [0,3]\n",
      "[0. 0. 0. 0.]\t<-- state [1,0]\n",
      "[0. 0. 0. 0.]\t<-- state [1,1]\n",
      "[0. 0. 0. 0.]\t<-- state [1,2]\n",
      "[0. 0. 0. 0.]\t<-- state [1,3]\n",
      "[0. 0. 0. 0.]\t<-- state [2,0]\n",
      "[0. 0. 0. 0.]\t<-- state [2,1]\n",
      "[0. 0. 0. 0.]\t<-- state [2,2]\n",
      "[0. 0. 0. 0.]\t<-- state [2,3]\n",
      "[0. 0. 0. 0.]\t<-- state [3,0]\n",
      "[0. 0. 0. 0.]\t<-- state [3,1]\n",
      "[0. 0. 0. 0.]\t<-- state [3,2]\n",
      "[0. 0. 0. 0.]\t<-- state [3,3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "# Print some info about it\n",
    "s = 'Q-table:\\n'\n",
    "i = 0\n",
    "j = 0\n",
    "for cr in q_table:\n",
    "    s += '{}\\t<-- state [{},{}]\\n'.format(cr, i, j)\n",
    "    j += 1\n",
    "    if j >= action_space_size:\n",
    "        j = 0\n",
    "        i += 1\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae01632",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44427ed4e698c56b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With an empty Q-table the agent do not know anything about what actions are better than others. In other words, the Q-value function will always return zero for every possible action at each state. Thus in the beginning the agent will instead be taking random actions while exploring the board. This is called *exploration*. However, after having gained some knowledge about the board, the agent can start relying on the experience gained and reflected in the Q-value function. This is called *exploitation*. We will be using the *epsilon greedy strategy* to find a balance between *exploration* (random actions) and *exploitation* (optional actions according to the Q-value function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62e5b02",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a5b92e348833746",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we introduce some parameters to use in the Q-training algorithm.\n",
    "\n",
    "# Learning rate (value between 0 and 1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Reward discount used in the Bellman equation (value between 0 and 1)\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44db8b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a040c9a8a6c9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Bellman Equation\n",
    "For updating a state in the Q-table we will here be using the following version of the *Bellman equation*:<br>\n",
    "$$Q^{updated}(s_{t}, a_{t}) := (1-\\alpha)Q(s_{t}, a_{t}) + \\alpha (r_{t} + \\gamma \\underset{a}{\\max}Q(s_{t+1},a))$$\n",
    "$s_{t}$ - current state.<br>\n",
    "$a_{t}$ - chosen action.<br>\n",
    "$\\alpha$ - learning rate.<br>\n",
    "$r_{t}$ - reward from taking the action $a_{t}$.<br>\n",
    "$\\gamma$ - discount factor.<br>\n",
    "$s_{t+1}$ - new state after taking action $a_{t}$.<br>\n",
    "$\\underset{a}{\\max}Q(s_{t+1},a)$ - the optional value possible from $s_{t+1}$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80054275",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da18419fc78cb4a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.4\n",
    "\n",
    "<strong><i><u>Your task<i></u></strong> is to implement the Bellman equation in `q_update()` that calculates and returns the new value for $Q^{updated}(s_{t}, a_{t})$.<br>It takes as input: Q-table, $s_{t}$, $a_{t}$, $s_{t+1}$, $r_{t}$, $\\alpha$, and $\\gamma$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1915777",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b8c88159a873820",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor):\n",
    "    # Calculate the new value in the q_table using Bellman equation\n",
    "\n",
    "    ## Implement the Bellman equation as shown above\n",
    "    # q_new = q_table[state, action] * ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    q_new = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc8915d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-789d006652c57c07",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "# Should this test be improved somehow?\n",
    "assert q_update(q_table=q_table, state=0, action=2, new_state=1, reward=0, learning_rate=learning_rate, discount_factor=discount_factor) == 0.0\n",
    "assert q_update(q_table=q_table, state=0, action=2, new_state=1, reward=1, learning_rate=learning_rate, discount_factor=discount_factor) == 0.1\n",
    "\n",
    "test_q_table = np.array([[0.58940116, 0.50458308, 0.50449804, 0.50430707],\n",
    "                [0.39701686, 0.31072888, 0.32279088, 0.52844835],\n",
    "                [0.40813145, 0.43028301, 0.41980598, 0.4987701],\n",
    "                [0.34803986, 0.31594787, 0.31342393, 0.47886813],\n",
    "                [0.6137157, 0.39051021, 0.33313007, 0.40253577],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45621221, 0.157744,  0.16167418, 0.15956471],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.40419588, 0.44695664, 0.49716483, 0.66166421],\n",
    "                [0.48304007, 0.70040278, 0.43290747, 0.42981004],\n",
    "                [0.68873522, 0.45787092, 0.36492453, 0.39246048],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45736412, 0.66062829, 0.80533251, 0.59763036],\n",
    "                [0.7685619,  0.9257641,  0.77140422, 0.75617869],\n",
    "                [0.,         0.,         0.,         0.        ]])\n",
    "assert 0.5 <= q_update(q_table=test_q_table, state=8, action=2, new_state=9, reward=0, learning_rate=learning_rate, discount_factor=discount_factor) <= 0.6 # == 0.51678822222\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374604c2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13c14052c56b4d4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# For the mentioned epsilon greedy strategy we will need a few more parameters.\n",
    "\n",
    "# Upper bound for the exploration rate (value between 0 and 1)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Lower bound for the exploration rate (value between 0 and 1)\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# How much the exploration rate decays per episode (down to the min_exploration_rate, value between 0 and 1).\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd219d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f94d61e0fd38afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Epsilon Greedy Strategy\n",
    "By using the *epsilon greedy strategy*, we will let the exploration rate start high (1), and then decrease for every episode with exponential decay. The goal here is to find the optimal balance between exploration and exploitation. Each round will consist of 10000 episodes. In the first episodes we want to have a high probility of performing exploration actions to avoid getting stuck in sub-optimal paths (local maximums). However, we want to gradually start leaning more towards exploitation towards the later episodes.<br>\n",
    "<br>\n",
    "We will be using the following formula for calculating the exploration rate for a given episode $\\epsilon(i)$:<br>\n",
    "$$\\epsilon(i) = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min}) \\cdot e^{-\\zeta \\cdot i}$$\n",
    "$\\epsilon$ - exploration rate.<br>\n",
    "$i$ - episode number.<br>\n",
    "$\\epsilon_{min}$ - min exploration rate.<br>\n",
    "$\\epsilon_{max}$ - max exploration rate.<br>\n",
    "$\\zeta$ - exploration decay rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcc3f5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd5341b2e6ce6da4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.5\n",
    "\n",
    "Implement the `calc_exploration_rate()` function which calculates and returns the exploration rate using the above formula.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210d77f4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d10be506d65f993",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculate the exploration rate for a given episode\n",
    "def calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate):\n",
    "    ## Calculate the exploration rate\n",
    "    # exploration_rate = ...\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    ### END SOLUTION\n",
    "    return exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df34e84c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-307f265b7d244b66",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert 0.03 <= calc_exploration_rate(700, 0.01, 1, 0.005) <= 0.04  # == 0.039895409588095315\n",
    "assert 0.55 <= calc_exploration_rate(100, 0.01, 1, 0.005) <= 0.65 # == 0.6104653531155071\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb551b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5007d85f0ea8352f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.6\n",
    "\n",
    "Next we will make the function for choosing an action given a state. This function includes an *exploration rate threshold* which is a randomly drawn number between 0 and 1. For every action, this thershold makes the agent perform an exploration action with a probability equal to the given *exploration rate*.\n",
    "<br>\n",
    "<strong><i><u>Your task<i></u></strong> is to complete the below function `choose_action()`. This function takes as input the Q-table, current state $s_{t}$, and exploration rate $\\epsilon$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f932326",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-188c8ae072cecd71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, exploration_rate):\n",
    "    \n",
    "    ## 1. Pick a random number between 0 and 1\n",
    "    # exploration_rate_threshold = ...\n",
    "    \n",
    "    ### BEGIN SOLUTION \n",
    "    exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    ## 2. Exploration vs exploitation\n",
    "    if exploration_rate > exploration_rate_threshold or sum(q_table[state, :]) == 0:\n",
    "        ## Exploration, i.e., random action \n",
    "        # action = ...\n",
    "        pass\n",
    "    else:\n",
    "        ## Exploitation, i.e., perform the optimal action at this state according to the Q-table\n",
    "        # action = ...\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    if exploration_rate > exploration_rate_threshold or sum(q_table[state, :]) == 0:\n",
    "        ## Exploration, i.e., random action \n",
    "        action = random.choice(range(len(['left', 'down', 'right', 'up'])))\n",
    "        # We could also use: action = env.action_space.sample()\n",
    "    else:\n",
    "        ## Exploitation, i.e., perform the optimal action at this state according to the Q-table\n",
    "        action = np.argmax(q_table[state, :])\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5411b02b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c5bab8432aab8bc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "test_q_table = np.array([[0.58940116, 0.50458308, 0.50449804, 0.50430707],\n",
    "                [0.39701686, 0.31072888, 0.32279088, 0.52844835],\n",
    "                [0.40813145, 0.43028301, 0.41980598, 0.4987701],\n",
    "                [0.34803986, 0.31594787, 0.31342393, 0.47886813],\n",
    "                [0.6137157, 0.39051021, 0.33313007, 0.40253577],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45621221, 0.157744,  0.16167418, 0.15956471],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.40419588, 0.44695664, 0.49716483, 0.66166421],\n",
    "                [0.48304007, 0.70040278, 0.43290747, 0.42981004],\n",
    "                [0.68873522, 0.45787092, 0.36492453, 0.39246048],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.,         0.,         0.,         0.        ],\n",
    "                [0.45736412, 0.66062829, 0.80533251, 0.59763036],\n",
    "                [0.7685619,  0.9257641,  0.77140422, 0.75617869],\n",
    "                [0.,         0.,         0.,         0.        ]])\n",
    "\n",
    "action_list = []\n",
    "for i in range(100):\n",
    "    i_action = choose_action(test_q_table, 3, 0)\n",
    "    action_list.append(i_action)\n",
    "assert action_list[0] == 3 and all(x == action_list[0] for x in action_list)\n",
    "\n",
    "action_list = []\n",
    "for i in range(100):\n",
    "    i_action = choose_action(test_q_table, 3, 1)\n",
    "    action_list.append(i_action)\n",
    "assert not all(x == action_list[0] for x in action_list)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80ede",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8de9391ad7cfd1d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Learning Algorithm\n",
    "Now it is time to train our agent using the Q-learning algorithm as shown below.\n",
    "If everything works as intended, the below code should print some statistics from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d85fe0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1047267499c396f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train\n",
    "num_episodes = 10000\n",
    "\n",
    "# Max number of steps before ending the round\n",
    "max_steps_per_episode = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0803db2f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c4b9b72e5654f6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Q-learning function\n",
    "def train_q_learn(env, q_table):\n",
    "    \n",
    "    # Q-Learning algorithm\n",
    "    rewards_all_episodes = []\n",
    "    steps_all_episodes = []\n",
    "    exploration_rate = max_exploration_rate\n",
    "\n",
    "    # Play the game for n episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        steps_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Chooce an action. Exploration vs exploitation trade-off with the epsilon greedy strategy\n",
    "            action = choose_action(q_table, state, exploration_rate)\n",
    "\n",
    "            # Perform the chosen action\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Upadete the Q-table\n",
    "            q_table[state, action] = q_update(q_table, state, action, new_state, reward, learning_rate, discount_factor)\n",
    "\n",
    "            # Set the new state\n",
    "            state = new_state\n",
    "\n",
    "            # Collect some statistics\n",
    "            rewards_current_episode += reward\n",
    "            steps_current_episode += 1\n",
    "\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        # Update the exploration rate\n",
    "        exploration_rate = calc_exploration_rate(episode, min_exploration_rate, max_exploration_rate, exploration_decay_rate)\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "        steps_all_episodes.append(steps_current_episode)\n",
    "\n",
    "\n",
    "    # Print the average rewards\n",
    "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "    steps_per_thousand_episodes = np.split(np.array(steps_all_episodes), num_episodes/1000)\n",
    "\n",
    "    print('Training results, average reward and steps per thousand episodes:')\n",
    "    count = 1000\n",
    "    for i, _ in enumerate(rewards_per_thousand_episodes):\n",
    "        r = rewards_per_thousand_episodes[i]\n",
    "        s = steps_per_thousand_episodes[i]\n",
    "        print('{}: rewards {:.4}, steps {:.4}'.format(count, sum(r/1000), sum(s/1000)))\n",
    "        count += 1000\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577d21b7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9c3f2482257ac56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results, average reward and steps per thousand episodes:\n",
      "1000: rewards 0.241, steps 7.824\n",
      "2000: rewards 0.709, steps 6.756\n",
      "3000: rewards 0.905, steps 6.35\n",
      "4000: rewards 0.97, steps 6.2\n",
      "5000: rewards 0.985, steps 6.08\n",
      "6000: rewards 0.984, steps 6.041\n",
      "7000: rewards 0.981, steps 6.003\n",
      "8000: rewards 0.985, steps 6.026\n",
      "9000: rewards 0.991, steps 6.034\n",
      "10000: rewards 0.993, steps 6.037\n"
     ]
    }
   ],
   "source": [
    "# Run the train_q_learn function\n",
    "q_table = train_q_learn(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7344263d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48dc2140c1871f01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[0.94148015 0.93206535 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099004 0.96059601]\n",
      " [0.96059601 0.         0.90290009 0.88414575]\n",
      " [0.86145443 0.65470573 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20360948 0.         0.87050746 0.59358885]\n",
      " [0.54423985 0.52190896 0.98009943 0.        ]\n",
      " [0.97028957 0.99       0.         0.97029895]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.38816159 0.98999971 0.63283664]\n",
      " [0.98009678 0.98999988 1.         0.98009925]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Trained Q-table:')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622d7c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5b976692e3ed316",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Playing Frozen Lake\n",
    "Finally we will let the agent play Frozen Lake using the Q-table that we have learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447edd02",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff4b832693eb4e4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.7\n",
    "\n",
    "Now we are ready to put the agent to navigate the frozen lake based on what it has learned. By simply looking at the Q-table, maybe you can already see what route the agent will be taking?<br><br>\n",
    "<strong><i><u>Your task<i></u></strong> is to run the below code and observe how the agent is navigating the frozen lake. Observe the learned Q-table to understand why the agent is taking this specific path.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3849f1a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73a58f39ab224524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_check():\n",
    "    nbgrader_exec_env = os.environ.get('NBGRADER_EXECUTION')\n",
    "    if nbgrader_exec_env is not None and nbgrader_exec_env in ['autograde', 'validate']:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Defining the run_agent function\n",
    "def run_agent(env, q_table):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    if run_check():\n",
    "        clear_output(wait=False)\n",
    "        # Watch the agent play for a few rounds using the learned Q-table\n",
    "        n_rounds = 3\n",
    "        for episode in range(n_rounds):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "\n",
    "            print('༺❁ EPISODE {} ❁༻'.format(episode+1))\n",
    "            time.sleep(1)\n",
    "\n",
    "            for step in range(max_steps_per_episode + 1):\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "                action = np.argmax(q_table[state, :])\n",
    "\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    clear_output(wait=True)\n",
    "                    env.render()\n",
    "                    if reward == 1:\n",
    "                        print('༺❁ You reached the goal after {} steps! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    elif step == max_steps_per_episode - 1:\n",
    "                        print('༺❁ No steps left ({} steps max)! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    else:\n",
    "                        print('༺❁ You fell through a hole after {} steps! ❁༻'.format(step + 1))\n",
    "                        time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                    break\n",
    "                elif step == max_steps_per_episode - 1:\n",
    "                    print('༺❁ No steps left ({} steps max)! ❁༻'.format(step + 1))\n",
    "                    time.sleep(3)\n",
    "                    clear_output(wait=True)\n",
    "                    break\n",
    "                state = new_state\n",
    "        time.sleep(3)\n",
    "        print('༺❁ Done ❁༻')\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "968397e4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72d1f2d298047633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༺❁ Done ❁༻\n"
     ]
    }
   ],
   "source": [
    "# Run the run_agent function with the trained Q-table\n",
    "run_agent(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d9514",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9a443a782d5348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Ice Can Get Slippery!\n",
    "![Slippery ice](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7iaMa5OcNHtJFKB1fZo_EvG-nQyPPTm_fsA&usqp=CAU \"Slippery ice\")\n",
    "\n",
    "So far the ice has been easy to walk on, and we have seen that the agent has not had any problem in finding an optimal path leading to the goal. Now, however, the weather has changed and the ice has suddenly become very slippery.\n",
    "In the next part of this exercise we have defined the frozen lake as:<br>\n",
    "`env_frozen = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")`<br>\n",
    "`is_slippery=True` means that there is only a 1/3 chance that the agent will move in the intended \n",
    "direction, and 2/3 chance to move in one of the perpendicular directions (1/3 and 1/3). \n",
    "E.g., if **action = left**, then:\n",
    "- P(move left) = 1/3\n",
    "- P(move up) = 1/3\n",
    "- P(move down) = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642b7f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c75890d0dc7b593e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "### Student Task A5.8a\n",
    "\n",
    "First run the Q-Learning algorithm below and observe how the new trained Q-table looks like, and how the new statistics has become. Also observe how the behaviour of the agent has changed at run time.\n",
    "    \n",
    "### Student Task A5.8b\n",
    "Next we want you to briefly explore the impact of different values assigned to the different parameters, and try to find the most promising values that optimize the reward gained by the agent (look at the scores from the last 1000 episodes).\n",
    "The parameters in question are:\n",
    "- learning_rate ($\\alpha$)\n",
    "- discount_factor ($\\gamma$)\n",
    "- max_exploration_rate ($\\epsilon_{max}$)\n",
    "- min_exploration_rate ($\\epsilon_{min}$)\n",
    "- exploration_decay_rate ($\\zeta$)\n",
    "\n",
    "<u>Be prepared to discuss your findings with your colleges and/or teaching assistants.</u><br>\n",
    "Some additional things to think about: Why is the preferred first move of the agent to move left? Can you think of some possible approaches and strategies that could improve the performance of the agent further? Can you think of some completely different ways for how to solve this game/problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b36984fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results, average reward and steps per thousand episodes:\n",
      "1000: rewards 0.05, steps 10.61\n",
      "2000: rewards 0.185, steps 19.48\n",
      "3000: rewards 0.385, steps 31.78\n",
      "4000: rewards 0.553, steps 38.11\n",
      "5000: rewards 0.666, steps 41.85\n",
      "6000: rewards 0.641, steps 41.28\n",
      "7000: rewards 0.676, steps 44.01\n",
      "8000: rewards 0.666, steps 44.43\n",
      "9000: rewards 0.667, steps 42.61\n",
      "10000: rewards 0.681, steps 43.85\n"
     ]
    }
   ],
   "source": [
    "# Initiate the FrozenLake game with slippery ice\n",
    "env_slippery = gym.make('FrozenLake-v1', is_slippery=True, map_name=\"4x4\")\n",
    "\n",
    "\n",
    "## Parameters used by the Q-training algorithm\n",
    "\n",
    "# Learning rate (value between 0 and 1)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Reward discount used in the Bellman equation (value between 0 and 1)\n",
    "discount_factor = 0.99\n",
    "\n",
    "# Upper bound for the exploration rate (value between 0 and 1)\n",
    "max_exploration_rate = 1.0\n",
    "\n",
    "# Lower bound for the exploration rate (value between 0 and 1)\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# How much the exploration rate decays per episode (down to the min_exploration_rate, value between 0 and 1).\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "\n",
    "# Train the agent to navigate the slippery ice\n",
    "q_table_slippery = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "if run_check():\n",
    "    q_table_slippery = train_q_learn(env_slippery, q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d794daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained slippery Q-table:\n",
      "[[0.5134824  0.51345952 0.51457988 0.51196782]\n",
      " [0.43025994 0.35943826 0.22928711 0.48553617]\n",
      " [0.42742716 0.4306471  0.41266646 0.47241104]\n",
      " [0.204221   0.21668389 0.37103227 0.46457886]\n",
      " [0.52409846 0.39190617 0.3696236  0.31057624]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3077389  0.16617661 0.25470386 0.14984885]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2695666  0.42672198 0.39492453 0.54662505]\n",
      " [0.48684508 0.58157936 0.33136771 0.3643115 ]\n",
      " [0.61467724 0.46435338 0.220711   0.15008846]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5417055  0.53800563 0.7130033  0.47462597]\n",
      " [0.72765755 0.91884932 0.77132084 0.73754496]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Trained slippery Q-table:')\n",
    "print(q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "274422be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e7ca1a874e66864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༺❁ Done ❁༻\n"
     ]
    }
   ],
   "source": [
    "# Run the agent to see how the agent now navigates the slippery ice.\n",
    "run_agent(env_slippery, q_table_slippery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e0e3f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35079d3f9bc1157c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
